{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cca701-bdf8-4389-ad68-23b05a59e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HW4: Train DCGAN, WGAN / WGAN‑GP, and ACGAN on CIFAR‑10 (PyTorch)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GANConfig:\n",
    "    model: str = \"dcgan\"              # dcgan | wgan | wgan-gp | acgan\n",
    "    data_root: str = \"./data\"\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 128\n",
    "    z_dim: int = 100\n",
    "    g_lr: float = 2e-4\n",
    "    d_lr: float = 2e-4\n",
    "    beta1: float = 0.5\n",
    "    beta2: float = 0.999\n",
    "    img_size: int = 64\n",
    "    channels: int = 3\n",
    "    critic_iters: int = 5            # WGAN(-GP)\n",
    "    clip_value: float = 0.01         # WGAN\n",
    "    gp_lambda: float = 10.0          # WGAN-GP\n",
    "    num_workers: int = 4\n",
    "    out_dir: str = \".\"\n",
    "    seed: int = 42\n",
    "    device: Optional[str] = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Utils\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def make_dirs(run_name, out_dir):\n",
    "    ckpt = os.path.join(out_dir, \"checkpoints\", run_name)\n",
    "    samp = os.path.join(out_dir, \"samples\", run_name)\n",
    "    os.makedirs(ckpt, exist_ok=True)\n",
    "    os.makedirs(samp, exist_ok=True)\n",
    "    return ckpt, samp\n",
    "\n",
    "\n",
    "def _save_loss_plot(curves: dict, out_path: str, title: str):\n",
    "    \"\"\"curves: {name: [per-epoch values]}\"\"\"\n",
    "    plt.figure()\n",
    "    for k, v in curves.items():\n",
    "        if v:\n",
    "            plt.plot(range(1, len(v)+1), v, label=k)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.savefig(out_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _append_csv_row(csv_file: str, headers: list, row: list):\n",
    "    exists = os.path.exists(csv_file)\n",
    "    with open(csv_file, 'a', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow(headers)\n",
    "        w.writerow(row)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Models\n",
    "# ----------------------------------------------------\n",
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, base*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(base*8), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*8, base*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*4, base*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*2, base, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z.view(z.size(0), z.size(1), 1, 1))\n",
    "\n",
    "\n",
    "class DCGANDiscriminator(nn.Module):\n",
    "    def __init__(self, channels=3, base=64, sigmoid_out=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(channels, base, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base, base*2, 4, 2, 1, bias=False), nn.BatchNorm2d(base*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*2, base*4, 4, 2, 1, bias=False), nn.BatchNorm2d(base*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*4, base*8, 4, 2, 1, bias=False), nn.BatchNorm2d(base*8), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*8, 1, 4, 1, 0, bias=False),\n",
    "        ]\n",
    "        if sigmoid_out:\n",
    "            layers.append(nn.Sigmoid())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "\n",
    "class ACGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, n_classes=10, channels=3, base=64, emb_dim=50):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_classes, emb_dim)\n",
    "        self.proj = nn.Linear(z_dim + emb_dim, z_dim)\n",
    "        self.g = DCGANGenerator(z_dim, channels, base)\n",
    "    def forward(self, z, y):\n",
    "        e = self.embed(y)\n",
    "        zc = self.proj(torch.cat([z, e], 1))\n",
    "        return self.g(zc)\n",
    "\n",
    "\n",
    "class ACGANDiscriminator(nn.Module):\n",
    "    def __init__(self, n_classes=10, channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(channels, base, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base, base*2, 4, 2, 1, bias=False), nn.BatchNorm2d(base*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*2, base*4, 4, 2, 1, bias=False), nn.BatchNorm2d(base*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*4, base*8, 4, 2, 1, bias=False), nn.BatchNorm2d(base*8), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.adv = nn.Sequential(nn.Conv2d(base*8, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "        self.cls = nn.Conv2d(base*8, n_classes, 4, 1, 0, bias=False)\n",
    "    def forward(self, x):\n",
    "        f = self.f(x)\n",
    "        return self.adv(f).view(-1), self.cls(f).view(x.size(0), -1)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Loss helpers\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def gradient_penalty(C, real, fake, device):\n",
    "    bs = real.size(0)\n",
    "    eps = torch.rand(bs, 1, 1, 1, device=device)\n",
    "    xhat = eps * real + (1 - eps) * fake\n",
    "    xhat.requires_grad_(True)\n",
    "    pred = C(xhat)\n",
    "    grads = torch.autograd.grad(pred, xhat, torch.ones_like(pred), True, True)[0]\n",
    "    return ((grads.view(bs, -1).norm(2, 1) - 1)**2).mean()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def get_loader(root, size, batch, workers):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(size), transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=True, download=True, transform=tfm)\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Training loops (DCGAN / WGAN / ACGAN)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, 'bias', None) is not None and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def train_dcgan(cfg):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = DCGANGenerator(cfg.z_dim).to(device)\n",
    "    D = DCGANDiscriminator().to(device)\n",
    "    G.apply(weights_init); D.apply(weights_init)\n",
    "    opt_g = optim.Adam(G.parameters(), lr=cfg.g_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=cfg.d_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    bce = nn.BCELoss()\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "\n",
    "    run_name = f\"dcgan_{cfg.img_size}\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        d_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, _ in loader:\n",
    "            real = real.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # --- D\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            fake = G(z).detach()\n",
    "            loss_d = (\n",
    "                bce(D(real), torch.ones(bs, device=device))\n",
    "                + bce(D(fake), torch.zeros(bs, device=device))\n",
    "            )\n",
    "            opt_d.zero_grad(); loss_d.backward(); opt_d.step()\n",
    "\n",
    "            # --- G\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            gen = G(z)\n",
    "            loss_g = bce(D(gen), torch.ones(bs, device=device))\n",
    "            opt_g.zero_grad(); loss_g.backward(); opt_g.step()\n",
    "\n",
    "            d_sum += loss_d.item(); g_sum += loss_g.item(); n_batches += 1\n",
    "\n",
    "        d_epoch = d_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        d_losses.append(d_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','D_loss','G_loss'], [ep, d_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[DCGAN] Ep{ep} D:{d_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        save_grid(G, fixed_z, samp, ep)\n",
    "        _save_loss_plot({'D': d_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), 'DCGAN Loss')\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(D.state_dict(), os.path.join(ckpt, f\"D_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'D': d_losses, 'G': g_losses}\n",
    "\n",
    "\n",
    "def train_wgan(cfg, gp=False):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = DCGANGenerator(cfg.z_dim).to(device)\n",
    "    C = DCGANDiscriminator(sigmoid_out=False).to(device)  # critic\n",
    "    G.apply(weights_init); C.apply(weights_init)\n",
    "\n",
    "    opt_g = optim.RMSprop(G.parameters(), lr=5e-5) if not gp else optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "    opt_c = optim.RMSprop(C.parameters(), lr=5e-5) if not gp else optim.Adam(C.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "    run_name = \"wgan-gp\" if gp else \"wgan\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    c_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        c_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, _ in loader:\n",
    "            real = real.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # critic updates\n",
    "            for _ in range(cfg.critic_iters):\n",
    "                z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "                fake = G(z).detach()\n",
    "                C.zero_grad()\n",
    "                loss_c = -(C(real).mean() - C(fake).mean())\n",
    "                if gp:\n",
    "                    loss_c = loss_c + cfg.gp_lambda * gradient_penalty(C, real, fake, device)\n",
    "                loss_c.backward(); opt_c.step()\n",
    "                if not gp:\n",
    "                    for p in C.parameters():\n",
    "                        p.data.clamp_(-cfg.clip_value, cfg.clip_value)\n",
    "\n",
    "            # generator update\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            G.zero_grad()\n",
    "            loss_g = -C(G(z)).mean()\n",
    "            loss_g.backward(); opt_g.step()\n",
    "\n",
    "            c_sum += loss_c.item(); g_sum += loss_g.item(); n_batches += 1\n",
    "\n",
    "        c_epoch = c_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        c_losses.append(c_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','Critic_loss','G_loss'], [ep, c_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[{'WGAN-GP' if gp else 'WGAN'}] Ep{ep} C:{c_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        save_grid(G, fixed_z, samp, ep)\n",
    "        _save_loss_plot({'Critic': c_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), f\"{'WGAN-GP' if gp else 'WGAN'} Loss\")\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(C.state_dict(), os.path.join(ckpt, f\"C_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'Critic': c_losses, 'G': g_losses}\n",
    "\n",
    "\n",
    "def train_acgan(cfg):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = ACGANGenerator(cfg.z_dim).to(device)\n",
    "    D = ACGANDiscriminator().to(device)\n",
    "    G.apply(weights_init); D.apply(weights_init)\n",
    "\n",
    "    opt_g = optim.Adam(G.parameters(), lr=cfg.g_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=cfg.d_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    adv_loss = nn.BCELoss(); cls_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "    fixed_y = torch.tensor([i % 10 for i in range(64)], dtype=torch.long, device=device)\n",
    "\n",
    "    run_name = \"acgan\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        d_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, labels in loader:\n",
    "            real = real.to(device); labels = labels.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # --- D: adv + aux cls\n",
    "            valid = torch.ones(bs, device=device); fakef = torch.zeros(bs, device=device)\n",
    "            D.zero_grad()\n",
    "            adv_r, cls_r = D(real)\n",
    "            d_adv_r = adv_loss(adv_r, valid)\n",
    "            d_cls_r = cls_loss(cls_r, labels)\n",
    "\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            y_fake = torch.randint(0, 10, (bs,), device=device)\n",
    "            x_fake = G(z, y_fake).detach()\n",
    "            adv_f, cls_f = D(x_fake)\n",
    "            d_adv_f = adv_loss(adv_f, fakef)\n",
    "            d_cls_f = cls_loss(cls_f, y_fake)\n",
    "\n",
    "            d_loss = d_adv_r + d_adv_f + 0.5 * (d_cls_r + d_cls_f)\n",
    "            d_loss.backward(); opt_d.step()\n",
    "\n",
    "            # --- G: fool + correct class\n",
    "            G.zero_grad()\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            y = torch.randint(0, 10, (bs,), device=device)\n",
    "            gen = G(z, y)\n",
    "            adv_p, cls_p = D(gen)\n",
    "            g_loss = adv_loss(adv_p, valid) + cls_loss(cls_p, y)\n",
    "            g_loss.backward(); opt_g.step()\n",
    "\n",
    "            d_sum += d_loss.item(); g_sum += g_loss.item(); n_batches += 1\n",
    "\n",
    "        d_epoch = d_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        d_losses.append(d_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','D_loss','G_loss'], [ep, d_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[ACGAN] Ep{ep} D:{d_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        with torch.no_grad():\n",
    "            fake_samples = G(fixed_z, fixed_y)\n",
    "            grid = utils.make_grid(fake_samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "            utils.save_image(grid, os.path.join(samp, f\"epoch_{ep:04d}.png\"))\n",
    "        _save_loss_plot({'D': d_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), 'ACGAN Loss')\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(D.state_dict(), os.path.join(ckpt, f\"D_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'D': d_losses, 'G': g_losses}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Misc helpers & tests\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, 'bias', None) is not None and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def save_grid(G, z, sample_dir, epoch):\n",
    "    with torch.no_grad():\n",
    "        fake = G(z)\n",
    "        grid = utils.make_grid(fake, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "        utils.save_image(grid, os.path.join(sample_dir, f\"epoch_{epoch:04d}.png\"))\n",
    "\n",
    "\n",
    "def self_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    z_dim, bs = 100, 16\n",
    "\n",
    "    G = DCGANGenerator(z_dim).to(device)\n",
    "    D = DCGANDiscriminator().to(device)\n",
    "    z = torch.randn(bs, z_dim, device=device)\n",
    "    x_fake = G(z)\n",
    "    assert x_fake.shape == (bs, 3, 64, 64)\n",
    "    assert D(x_fake).shape == (bs,)\n",
    "\n",
    "    C = DCGANDiscriminator(sigmoid_out=False).to(device)\n",
    "    assert C(x_fake).shape == (bs,)\n",
    "\n",
    "    Gc = ACGANGenerator(z_dim).to(device)\n",
    "    Dc = ACGANDiscriminator().to(device)\n",
    "    y = torch.randint(0, 10, (bs,), device=device)\n",
    "    x_fake_c = Gc(z, y)\n",
    "    adv, cls = Dc(x_fake_c)\n",
    "    assert x_fake_c.shape == (bs, 3, 64, 64)\n",
    "    assert adv.shape == (bs,)\n",
    "    assert cls.shape == (bs, 10)\n",
    "\n",
    "    print(\"[SELF-TEST] All shapes OK.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Main (Jupyter/Colab friendly)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "import sys\n",
    "\n",
    "def get_parser():\n",
    "    p = argparse.ArgumentParser(add_help=True)\n",
    "    p.add_argument('--model', type=str, choices=['dcgan', 'wgan', 'wgan-gp', 'acgan'], default='dcgan')\n",
    "    p.add_argument('--data-root', type=str, default='./data')\n",
    "    p.add_argument('--epochs', type=int, default=50)\n",
    "    p.add_argument('--batch-size', type=int, default=128)\n",
    "    p.add_argument('--z-dim', type=int, default=100)\n",
    "    p.add_argument('--g-lr', type=float, default=2e-4)\n",
    "    p.add_argument('--d-lr', type=float, default=2e-4)\n",
    "    p.add_argument('--beta1', type=float, default=0.5)\n",
    "    p.add_argument('--beta2', type=float, default=0.999)\n",
    "    p.add_argument('--img-size', type=int, default=64)\n",
    "    p.add_argument('--critic-iters', type=int, default=5)\n",
    "    p.add_argument('--clip', dest='clip_value', type=float, default=0.01)\n",
    "    p.add_argument('--gp', dest='gp_lambda', type=float, default=10.0)\n",
    "    p.add_argument('--num-workers', type=int, default=4)\n",
    "    p.add_argument('--out-dir', type=str, default='.')\n",
    "    p.add_argument('--seed', type=int, default=42)\n",
    "    p.add_argument('--device', type=str, default=None)\n",
    "    p.add_argument('--self-test', action='store_true')\n",
    "    return p\n",
    "\n",
    "\n",
    "def _cfg_from_namespace(ns: argparse.Namespace) -> GANConfig:\n",
    "    return GANConfig(\n",
    "        model=ns.model,\n",
    "        data_root=ns.data_root,\n",
    "        epochs=ns.epochs,\n",
    "        batch_size=ns.batch_size,\n",
    "        z_dim=ns.z_dim,\n",
    "        g_lr=ns.g_lr,\n",
    "        d_lr=ns.d_lr,\n",
    "        beta1=ns.beta1,\n",
    "        beta2=ns.beta2,\n",
    "        img_size=ns.img_size,\n",
    "        critic_iters=ns.critic_iters,\n",
    "        clip_value=ns.clip_value,\n",
    "        gp_lambda=ns.gp_lambda,\n",
    "        num_workers=ns.num_workers,\n",
    "        out_dir=ns.out_dir,\n",
    "        seed=ns.seed,\n",
    "        device=ns.device,\n",
    "    )\n",
    "\n",
    "\n",
    "def run(model: str='dcgan', data_root: str='./data', epochs: int=50, batch_size: int=128,\n",
    "        z_dim: int=100, g_lr: float=2e-4, d_lr: float=2e-4, beta1: float=0.5, beta2: float=0.999,\n",
    "        img_size: int=64, critic_iters: int=5, clip_value: float=0.01, gp_lambda: float=10.0,\n",
    "        num_workers: int=4, out_dir: str='.', seed: int=42, device: Optional[str]=None,\n",
    "        self_test_flag: bool=False):\n",
    "    \"\"\"Notebook-friendly entry point. Call this from a Jupyter cell.\n",
    "\n",
    "    Example:\n",
    "        run('dcgan', epochs=1, batch_size=64)\n",
    "        run('wgan', epochs=1, critic_iters=5, clip_value=0.01)\n",
    "        run('acgan', epochs=1)\n",
    "        run(self_test_flag=True)\n",
    "    \"\"\"\n",
    "    args = argparse.Namespace(\n",
    "        model=model, data_root=data_root, epochs=epochs, batch_size=batch_size,\n",
    "        z_dim=z_dim, g_lr=g_lr, d_lr=d_lr, beta1=beta1, beta2=beta2,\n",
    "        img_size=img_size, critic_iters=critic_iters, clip_value=clip_value,\n",
    "        gp_lambda=gp_lambda, num_workers=num_workers, out_dir=out_dir,\n",
    "        seed=seed, device=device, self_test=self_test_flag\n",
    "    )\n",
    "    cfg = _cfg_from_namespace(args)\n",
    "    set_seed(cfg.seed)\n",
    "    if args.self_test:\n",
    "        self_test();\n",
    "        return None, {}\n",
    "    if cfg.model == 'dcgan':\n",
    "        return train_dcgan(cfg)\n",
    "    elif cfg.model == 'wgan':\n",
    "        return train_wgan(cfg, gp=False)\n",
    "    elif cfg.model == 'wgan-gp':\n",
    "        return train_wgan(cfg, gp=True)\n",
    "    elif cfg.model == 'acgan':\n",
    "        return train_acgan(cfg)\n",
    "    else:\n",
    "        raise ValueError(cfg.model)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = get_parser()\n",
    "    # In Jupyter, IPython passes extra flags like -f <kernel.json>.\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.self_test:\n",
    "        set_seed(0); self_test(); return\n",
    "\n",
    "    cfg = _cfg_from_namespace(args)\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if cfg.model == 'dcgan':\n",
    "        run_name, curves = train_dcgan(cfg)\n",
    "    elif cfg.model == 'wgan':\n",
    "        run_name, curves = train_wgan(cfg, gp=False)\n",
    "    elif cfg.model == 'wgan-gp':\n",
    "        run_name, curves = train_wgan(cfg, gp=True)\n",
    "    elif cfg.model == 'acgan':\n",
    "        run_name, curves = train_acgan(cfg)\n",
    "    else:\n",
    "        raise ValueError(cfg.model)\n",
    "    # Single-run curve already saved within trainer.\n",
    "\n",
    "\n",
    "# Removed auto-execution block to prevent automatic training when running the cell in Jupyter.\n",
    "# if __name__ == '__main__':\n",
    "#     if 'ipykernel' in sys.modules:\n",
    "#         sys.argv = [sys.argv[0]]\n",
    "#     main()\n",
    "\n",
    "# -------- Notebook helpers for combined plots --------\n",
    "\n",
    "def combined_loss_plot(models_curves: dict, out_path: str = 'samples/combined/loss_curve.png'):\n",
    "    \"\"\"\n",
    "    models_curves: {\n",
    "        'dcgan': {'G': [...], 'D': [...]},\n",
    "        'wgan':  {'G': [...], 'Critic': [...]},\n",
    "        'wgan-gp': {'G': [...], 'Critic': [...]},\n",
    "        'acgan': {'G': [...], 'D': [...]}\n",
    "    }\n",
    "    Saves a 1x2 figure: left=Generator losses, right=Discriminator/Critic losses.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    # Generator\n",
    "    for name, curves in models_curves.items():\n",
    "        g = curves.get('G', [])\n",
    "        if g:\n",
    "            axes[0].plot(range(1, len(g)+1), g, label=name)\n",
    "    axes[0].set_title('Generator loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
    "\n",
    "    # Discriminator / Critic\n",
    "    for name, curves in models_curves.items():\n",
    "        d = curves.get('D', [])\n",
    "        c = curves.get('Critic', [])\n",
    "        y = d if d else c\n",
    "        if y:\n",
    "            axes[1].plot(range(1, len(y)+1), y, label=name)\n",
    "    axes[1].set_title('Discriminator / Critic loss')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss'); axes[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.savefig(out_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def capture_run(model: str, **kwargs):\n",
    "    \"\"\"Run a training job and return only the per-epoch loss curves.\n",
    "    Example:\n",
    "        dc = capture_run('dcgan', epochs=10, num_workers=1)\n",
    "    \"\"\"\n",
    "    _, curves = run(model=model, **kwargs)\n",
    "    return curves\n",
    "\n",
    "\n",
    "def run_all_and_plot(epochs=10, num_workers=1, out_dir='.'):\n",
    "    \"\"\"Convenience: run DCGAN, WGAN, and ACGAN back-to-back and save a combined plot.\"\"\"\n",
    "    curves = {}\n",
    "    # Reduce workers to avoid dataloader freeze on shared systems\n",
    "    curves['dcgan'] = capture_run('dcgan', epochs=epochs, num_workers=num_workers, out_dir=out_dir)\n",
    "    curves['wgan']  = capture_run('wgan',  epochs=epochs, num_workers=num_workers, out_dir=out_dir, critic_iters=5, clip_value=0.01)\n",
    "    curves['acgan'] = capture_run('acgan', epochs=epochs, num_workers=num_workers, out_dir=out_dir)\n",
    "    out_path = os.path.join(out_dir, 'samples', 'combined', 'loss_curve.png')\n",
    "    combined_loss_plot(curves, out_path)\n",
    "    print(f\"Saved combined loss plot to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771b303-fe8e-43dd-95a4-0639dd0da3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b87e2b-70d3-4b32-a152-a286a7656c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = capture_run(\"dcgan\", epochs=10, num_workers=1)\n",
    "wg = capture_run(\"wgan\", epochs=10, num_workers=1)\n",
    "ac = capture_run(\"acgan\", epochs=10, num_workers=1)\n",
    "\n",
    "combined_loss_plot(\n",
    "    {\"dcgan\": dc, \"wgan\": wg, \"acgan\": ac},\n",
    "    out_path=\"samples/combined/loss_curve.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33f194-d61e-4e6a-877a-ea299da43fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run this cell if you want to see DCGAN epochs else the above 2 ceells are enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3a5bd-4391-48e5-8302-5f1298361609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GANConfig:\n",
    "    model: str = \"dcgan\"              # dcgan | wgan | wgan-gp | acgan\n",
    "    data_root: str = \"./data\"\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 128\n",
    "    z_dim: int = 100\n",
    "    g_lr: float = 2e-4\n",
    "    d_lr: float = 2e-4\n",
    "    beta1: float = 0.5\n",
    "    beta2: float = 0.999\n",
    "    img_size: int = 64\n",
    "    channels: int = 3\n",
    "    critic_iters: int = 5            # WGAN(-GP)\n",
    "    clip_value: float = 0.01         # WGAN\n",
    "    gp_lambda: float = 10.0          # WGAN-GP\n",
    "    num_workers: int = 4\n",
    "    out_dir: str = \".\"\n",
    "    seed: int = 42\n",
    "    device: Optional[str] = None\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Utils\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def set_seed(seed):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def make_dirs(run_name, out_dir):\n",
    "    ckpt = os.path.join(out_dir, \"checkpoints\", run_name)\n",
    "    samp = os.path.join(out_dir, \"samples\", run_name)\n",
    "    os.makedirs(ckpt, exist_ok=True)\n",
    "    os.makedirs(samp, exist_ok=True)\n",
    "    return ckpt, samp\n",
    "\n",
    "\n",
    "def _save_loss_plot(curves: dict, out_path: str, title: str):\n",
    "    \"\"\"curves: {name: [per-epoch values]}\"\"\"\n",
    "    plt.figure()\n",
    "    for k, v in curves.items():\n",
    "        if v:\n",
    "            plt.plot(range(1, len(v)+1), v, label=k)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.savefig(out_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _append_csv_row(csv_file: str, headers: list, row: list):\n",
    "    exists = os.path.exists(csv_file)\n",
    "    with open(csv_file, 'a', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow(headers)\n",
    "        w.writerow(row)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Models\n",
    "# ----------------------------------------------------\n",
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, base*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(base*8), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*8, base*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*4, base*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base*2, base, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(base, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z.view(z.size(0), z.size(1), 1, 1))\n",
    "\n",
    "\n",
    "class DCGANDiscriminator(nn.Module):\n",
    "    def __init__(self, channels=3, base=64, sigmoid_out=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(channels, base, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base, base*2, 4, 2, 1, bias=False), nn.BatchNorm2d(base*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*2, base*4, 4, 2, 1, bias=False), nn.BatchNorm2d(base*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*4, base*8, 4, 2, 1, bias=False), nn.BatchNorm2d(base*8), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*8, 1, 4, 1, 0, bias=False),\n",
    "        ]\n",
    "        if sigmoid_out:\n",
    "            layers.append(nn.Sigmoid())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "\n",
    "class ACGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, n_classes=10, channels=3, base=64, emb_dim=50):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_classes, emb_dim)\n",
    "        self.proj = nn.Linear(z_dim + emb_dim, z_dim)\n",
    "        self.g = DCGANGenerator(z_dim, channels, base)\n",
    "    def forward(self, z, y):\n",
    "        e = self.embed(y)\n",
    "        zc = self.proj(torch.cat([z, e], 1))\n",
    "        return self.g(zc)\n",
    "\n",
    "\n",
    "class ACGANDiscriminator(nn.Module):\n",
    "    def __init__(self, n_classes=10, channels=3, base=64):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(channels, base, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base, base*2, 4, 2, 1, bias=False), nn.BatchNorm2d(base*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*2, base*4, 4, 2, 1, bias=False), nn.BatchNorm2d(base*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(base*4, base*8, 4, 2, 1, bias=False), nn.BatchNorm2d(base*8), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.adv = nn.Sequential(nn.Conv2d(base*8, 1, 4, 1, 0, bias=False), nn.Sigmoid())\n",
    "        self.cls = nn.Conv2d(base*8, n_classes, 4, 1, 0, bias=False)\n",
    "    def forward(self, x):\n",
    "        f = self.f(x)\n",
    "        return self.adv(f).view(-1), self.cls(f).view(x.size(0), -1)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Loss helpers\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def gradient_penalty(C, real, fake, device):\n",
    "    bs = real.size(0)\n",
    "    eps = torch.rand(bs, 1, 1, 1, device=device)\n",
    "    xhat = eps * real + (1 - eps) * fake\n",
    "    xhat.requires_grad_(True)\n",
    "    pred = C(xhat)\n",
    "    grads = torch.autograd.grad(pred, xhat, torch.ones_like(pred), True, True)[0]\n",
    "    return ((grads.view(bs, -1).norm(2, 1) - 1)**2).mean()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def get_loader(root, size, batch, workers):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(size), transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=True, download=True, transform=tfm)\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Training loops (DCGAN / WGAN / ACGAN)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, 'bias', None) is not None and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def train_dcgan(cfg):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = DCGANGenerator(cfg.z_dim).to(device)\n",
    "    D = DCGANDiscriminator().to(device)\n",
    "    G.apply(weights_init); D.apply(weights_init)\n",
    "    opt_g = optim.Adam(G.parameters(), lr=cfg.g_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=cfg.d_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    bce = nn.BCELoss()\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "\n",
    "    run_name = f\"dcgan_{cfg.img_size}\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        d_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, _ in loader:\n",
    "            real = real.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # --- D\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            fake = G(z).detach()\n",
    "            loss_d = (\n",
    "                bce(D(real), torch.ones(bs, device=device))\n",
    "                + bce(D(fake), torch.zeros(bs, device=device))\n",
    "            )\n",
    "            opt_d.zero_grad(); loss_d.backward(); opt_d.step()\n",
    "\n",
    "            # --- G\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            gen = G(z)\n",
    "            loss_g = bce(D(gen), torch.ones(bs, device=device))\n",
    "            opt_g.zero_grad(); loss_g.backward(); opt_g.step()\n",
    "\n",
    "            d_sum += loss_d.item(); g_sum += loss_g.item(); n_batches += 1\n",
    "\n",
    "        d_epoch = d_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        d_losses.append(d_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','D_loss','G_loss'], [ep, d_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[DCGAN] Ep{ep} D:{d_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        save_grid(G, fixed_z, samp, ep)\n",
    "        _save_loss_plot({'D': d_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), 'DCGAN Loss')\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(D.state_dict(), os.path.join(ckpt, f\"D_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'D': d_losses, 'G': g_losses}\n",
    "\n",
    "\n",
    "def train_wgan(cfg, gp=False):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = DCGANGenerator(cfg.z_dim).to(device)\n",
    "    C = DCGANDiscriminator(sigmoid_out=False).to(device)  # critic\n",
    "    G.apply(weights_init); C.apply(weights_init)\n",
    "\n",
    "    opt_g = optim.RMSprop(G.parameters(), lr=5e-5) if not gp else optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "    opt_c = optim.RMSprop(C.parameters(), lr=5e-5) if not gp else optim.Adam(C.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "    run_name = \"wgan-gp\" if gp else \"wgan\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    c_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        c_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, _ in loader:\n",
    "            real = real.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # critic updates\n",
    "            for _ in range(cfg.critic_iters):\n",
    "                z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "                fake = G(z).detach()\n",
    "                C.zero_grad()\n",
    "                loss_c = -(C(real).mean() - C(fake).mean())\n",
    "                if gp:\n",
    "                    loss_c = loss_c + cfg.gp_lambda * gradient_penalty(C, real, fake, device)\n",
    "                loss_c.backward(); opt_c.step()\n",
    "                if not gp:\n",
    "                    for p in C.parameters():\n",
    "                        p.data.clamp_(-cfg.clip_value, cfg.clip_value)\n",
    "\n",
    "            # generator update\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            G.zero_grad()\n",
    "            loss_g = -C(G(z)).mean()\n",
    "            loss_g.backward(); opt_g.step()\n",
    "\n",
    "            c_sum += loss_c.item(); g_sum += loss_g.item(); n_batches += 1\n",
    "\n",
    "        c_epoch = c_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        c_losses.append(c_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','Critic_loss','G_loss'], [ep, c_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[{'WGAN-GP' if gp else 'WGAN'}] Ep{ep} C:{c_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        save_grid(G, fixed_z, samp, ep)\n",
    "        _save_loss_plot({'Critic': c_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), f\"{'WGAN-GP' if gp else 'WGAN'} Loss\")\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(C.state_dict(), os.path.join(ckpt, f\"C_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'Critic': c_losses, 'G': g_losses}\n",
    "\n",
    "\n",
    "def train_acgan(cfg):\n",
    "    device = torch.device(cfg.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    loader = get_loader(cfg.data_root, cfg.img_size, cfg.batch_size, cfg.num_workers)\n",
    "    G = ACGANGenerator(cfg.z_dim).to(device)\n",
    "    D = ACGANDiscriminator().to(device)\n",
    "    G.apply(weights_init); D.apply(weights_init)\n",
    "\n",
    "    opt_g = optim.Adam(G.parameters(), lr=cfg.g_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=cfg.d_lr, betas=(cfg.beta1, cfg.beta2))\n",
    "    adv_loss = nn.BCELoss(); cls_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    fixed_z = torch.randn(64, cfg.z_dim, device=device)\n",
    "    fixed_y = torch.tensor([i % 10 for i in range(64)], dtype=torch.long, device=device)\n",
    "\n",
    "    run_name = \"acgan\"\n",
    "    ckpt, samp = make_dirs(run_name, cfg.out_dir)\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "    csv_path = os.path.join(samp, 'loss_log.csv')\n",
    "\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        d_sum = 0.0; g_sum = 0.0; n_batches = 0\n",
    "        for real, labels in loader:\n",
    "            real = real.to(device); labels = labels.to(device)\n",
    "            bs = real.size(0)\n",
    "\n",
    "            # --- D: adv + aux cls\n",
    "            valid = torch.ones(bs, device=device); fakef = torch.zeros(bs, device=device)\n",
    "            D.zero_grad()\n",
    "            adv_r, cls_r = D(real)\n",
    "            d_adv_r = adv_loss(adv_r, valid)\n",
    "            d_cls_r = cls_loss(cls_r, labels)\n",
    "\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            y_fake = torch.randint(0, 10, (bs,), device=device)\n",
    "            x_fake = G(z, y_fake).detach()\n",
    "            adv_f, cls_f = D(x_fake)\n",
    "            d_adv_f = adv_loss(adv_f, fakef)\n",
    "            d_cls_f = cls_loss(cls_f, y_fake)\n",
    "\n",
    "            d_loss = d_adv_r + d_adv_f + 0.5 * (d_cls_r + d_cls_f)\n",
    "            d_loss.backward(); opt_d.step()\n",
    "\n",
    "            # --- G: fool + correct class\n",
    "            G.zero_grad()\n",
    "            z = torch.randn(bs, cfg.z_dim, device=device)\n",
    "            y = torch.randint(0, 10, (bs,), device=device)\n",
    "            gen = G(z, y)\n",
    "            adv_p, cls_p = D(gen)\n",
    "            g_loss = adv_loss(adv_p, valid) + cls_loss(cls_p, y)\n",
    "            g_loss.backward(); opt_g.step()\n",
    "\n",
    "            d_sum += d_loss.item(); g_sum += g_loss.item(); n_batches += 1\n",
    "\n",
    "        d_epoch = d_sum / max(1, n_batches)\n",
    "        g_epoch = g_sum / max(1, n_batches)\n",
    "        d_losses.append(d_epoch); g_losses.append(g_epoch)\n",
    "        _append_csv_row(csv_path, ['epoch','D_loss','G_loss'], [ep, d_epoch, g_epoch])\n",
    "\n",
    "        print(f\"[ACGAN] Ep{ep} D:{d_epoch:.4f} G:{g_epoch:.4f}\")\n",
    "        with torch.no_grad():\n",
    "            fake_samples = G(fixed_z, fixed_y)\n",
    "            grid = utils.make_grid(fake_samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "            utils.save_image(grid, os.path.join(samp, f\"epoch_{ep:04d}.png\"))\n",
    "        _save_loss_plot({'D': d_losses, 'G': g_losses}, os.path.join(samp, 'loss_curve.png'), 'ACGAN Loss')\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt, f\"G_{ep:04d}.pt\"))\n",
    "        torch.save(D.state_dict(), os.path.join(ckpt, f\"D_{ep:04d}.pt\"))\n",
    "\n",
    "    return run_name, {'D': d_losses, 'G': g_losses}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Misc helpers & tests\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, 'bias', None) is not None and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def save_grid(G, z, sample_dir, epoch):\n",
    "    with torch.no_grad():\n",
    "        fake = G(z)\n",
    "        grid = utils.make_grid(fake, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "        utils.save_image(grid, os.path.join(sample_dir, f\"epoch_{epoch:04d}.png\"))\n",
    "\n",
    "\n",
    "def self_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    z_dim, bs = 100, 16\n",
    "\n",
    "    G = DCGANGenerator(z_dim).to(device)\n",
    "    D = DCGANDiscriminator().to(device)\n",
    "    z = torch.randn(bs, z_dim, device=device)\n",
    "    x_fake = G(z)\n",
    "    assert x_fake.shape == (bs, 3, 64, 64)\n",
    "    assert D(x_fake).shape == (bs,)\n",
    "\n",
    "    C = DCGANDiscriminator(sigmoid_out=False).to(device)\n",
    "    assert C(x_fake).shape == (bs,)\n",
    "\n",
    "    Gc = ACGANGenerator(z_dim).to(device)\n",
    "    Dc = ACGANDiscriminator().to(device)\n",
    "    y = torch.randint(0, 10, (bs,), device=device)\n",
    "    x_fake_c = Gc(z, y)\n",
    "    adv, cls = Dc(x_fake_c)\n",
    "    assert x_fake_c.shape == (bs, 3, 64, 64)\n",
    "    assert adv.shape == (bs,)\n",
    "    assert cls.shape == (bs, 10)\n",
    "\n",
    "    print(\"[SELF-TEST] All shapes OK.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Main (Jupyter/Colab friendly)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "import sys\n",
    "\n",
    "def get_parser():\n",
    "    p = argparse.ArgumentParser(add_help=True)\n",
    "    p.add_argument('--model', type=str, choices=['dcgan', 'wgan', 'wgan-gp', 'acgan'], default='dcgan')\n",
    "    p.add_argument('--data-root', type=str, default='./data')\n",
    "    p.add_argument('--epochs', type=int, default=50)\n",
    "    p.add_argument('--batch-size', type=int, default=128)\n",
    "    p.add_argument('--z-dim', type=int, default=100)\n",
    "    p.add_argument('--g-lr', type=float, default=2e-4)\n",
    "    p.add_argument('--d-lr', type=float, default=2e-4)\n",
    "    p.add_argument('--beta1', type=float, default=0.5)\n",
    "    p.add_argument('--beta2', type=float, default=0.999)\n",
    "    p.add_argument('--img-size', type=int, default=64)\n",
    "    p.add_argument('--critic-iters', type=int, default=5)\n",
    "    p.add_argument('--clip', dest='clip_value', type=float, default=0.01)\n",
    "    p.add_argument('--gp', dest='gp_lambda', type=float, default=10.0)\n",
    "    p.add_argument('--num-workers', type=int, default=4)\n",
    "    p.add_argument('--out-dir', type=str, default='.')\n",
    "    p.add_argument('--seed', type=int, default=42)\n",
    "    p.add_argument('--device', type=str, default=None)\n",
    "    p.add_argument('--self-test', action='store_true')\n",
    "    return p\n",
    "\n",
    "\n",
    "def _cfg_from_namespace(ns: argparse.Namespace) -> GANConfig:\n",
    "    return GANConfig(\n",
    "        model=ns.model,\n",
    "        data_root=ns.data_root,\n",
    "        epochs=ns.epochs,\n",
    "        batch_size=ns.batch_size,\n",
    "        z_dim=ns.z_dim,\n",
    "        g_lr=ns.g_lr,\n",
    "        d_lr=ns.d_lr,\n",
    "        beta1=ns.beta1,\n",
    "        beta2=ns.beta2,\n",
    "        img_size=ns.img_size,\n",
    "        critic_iters=ns.critic_iters,\n",
    "        clip_value=ns.clip_value,\n",
    "        gp_lambda=ns.gp_lambda,\n",
    "        num_workers=ns.num_workers,\n",
    "        out_dir=ns.out_dir,\n",
    "        seed=ns.seed,\n",
    "        device=ns.device,\n",
    "    )\n",
    "\n",
    "\n",
    "def run(model: str='dcgan', data_root: str='./data', epochs: int=50, batch_size: int=128,\n",
    "        z_dim: int=100, g_lr: float=2e-4, d_lr: float=2e-4, beta1: float=0.5, beta2: float=0.999,\n",
    "        img_size: int=64, critic_iters: int=5, clip_value: float=0.01, gp_lambda: float=10.0,\n",
    "        num_workers: int=4, out_dir: str='.', seed: int=42, device: Optional[str]=None,\n",
    "        self_test_flag: bool=False):\n",
    "    \"\"\"Notebook-friendly entry point. Call this from a Jupyter cell.\n",
    "\n",
    "    Example:\n",
    "        run('dcgan', epochs=1, batch_size=64)\n",
    "        run('wgan', epochs=1, critic_iters=5, clip_value=0.01)\n",
    "        run('acgan', epochs=1)\n",
    "        run(self_test_flag=True)\n",
    "    \"\"\"\n",
    "    args = argparse.Namespace(\n",
    "        model=model, data_root=data_root, epochs=epochs, batch_size=batch_size,\n",
    "        z_dim=z_dim, g_lr=g_lr, d_lr=d_lr, beta1=beta1, beta2=beta2,\n",
    "        img_size=img_size, critic_iters=critic_iters, clip_value=clip_value,\n",
    "        gp_lambda=gp_lambda, num_workers=num_workers, out_dir=out_dir,\n",
    "        seed=seed, device=device, self_test=self_test_flag\n",
    "    )\n",
    "    cfg = _cfg_from_namespace(args)\n",
    "    set_seed(cfg.seed)\n",
    "    if args.self_test:\n",
    "        self_test();\n",
    "        return None, {}\n",
    "    if cfg.model == 'dcgan':\n",
    "        return train_dcgan(cfg)\n",
    "    elif cfg.model == 'wgan':\n",
    "        return train_wgan(cfg, gp=False)\n",
    "    elif cfg.model == 'wgan-gp':\n",
    "        return train_wgan(cfg, gp=True)\n",
    "    elif cfg.model == 'acgan':\n",
    "        return train_acgan(cfg)\n",
    "    else:\n",
    "        raise ValueError(cfg.model)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = get_parser()\n",
    "    # In Jupyter, IPython passes extra flags like -f <kernel.json>.\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.self_test:\n",
    "        set_seed(0); self_test(); return\n",
    "\n",
    "    cfg = _cfg_from_namespace(args)\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if cfg.model == 'dcgan':\n",
    "        run_name, curves = train_dcgan(cfg)\n",
    "    elif cfg.model == 'wgan':\n",
    "        run_name, curves = train_wgan(cfg, gp=False)\n",
    "    elif cfg.model == 'wgan-gp':\n",
    "        run_name, curves = train_wgan(cfg, gp=True)\n",
    "    elif cfg.model == 'acgan':\n",
    "        run_name, curves = train_acgan(cfg)\n",
    "    else:\n",
    "        raise ValueError(cfg.model)\n",
    "    # Single-run curve already saved within trainer.\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        sys.argv = [sys.argv[0]]  # ignore notebook args like -f kernel.json\n",
    "    main()\n",
    "\n",
    "# -------- Notebook helpers for combined plots --------\n",
    "\n",
    "def combined_loss_plot(models_curves: dict, out_path: str = 'samples/combined/loss_curve.png'):\n",
    "    \"\"\"\n",
    "    models_curves: {\n",
    "        'dcgan': {'G': [...], 'D': [...]},\n",
    "        'wgan':  {'G': [...], 'Critic': [...]},\n",
    "        'wgan-gp': {'G': [...], 'Critic': [...]},\n",
    "        'acgan': {'G': [...], 'D': [...]}\n",
    "    }\n",
    "    Saves a 1x2 figure: left=Generator losses, right=Discriminator/Critic losses.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    # Generator\n",
    "    for name, curves in models_curves.items():\n",
    "        g = curves.get('G', [])\n",
    "        if g:\n",
    "            axes[0].plot(range(1, len(g)+1), g, label=name)\n",
    "    axes[0].set_title('Generator loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].legend()\n",
    "\n",
    "    # Discriminator / Critic\n",
    "    for name, curves in models_curves.items():\n",
    "        d = curves.get('D', [])\n",
    "        c = curves.get('Critic', [])\n",
    "        y = d if d else c\n",
    "        if y:\n",
    "            axes[1].plot(range(1, len(y)+1), y, label=name)\n",
    "    axes[1].set_title('Discriminator / Critic loss')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss'); axes[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    plt.savefig(out_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def capture_run(model: str, **kwargs):\n",
    "    \"\"\"Run a training job and return only the per-epoch loss curves.\n",
    "    Example:\n",
    "        dc = capture_run('dcgan', epochs=10, num_workers=1)\n",
    "    \"\"\"\n",
    "    _, curves = run(model=model, **kwargs)\n",
    "    return curves\n",
    "\n",
    "\n",
    "def run_all_and_plot(epochs=10, num_workers=1, out_dir='.'):\n",
    "    \"\"\"Convenience: run DCGAN, WGAN, and ACGAN back-to-back and save a combined plot.\"\"\"\n",
    "    curves = {}\n",
    "    # Reduce workers to avoid dataloader freeze on shared systems\n",
    "    curves['dcgan'] = capture_run('dcgan', epochs=epochs, num_workers=num_workers, out_dir=out_dir)\n",
    "    curves['wgan']  = capture_run('wgan',  epochs=epochs, num_workers=num_workers, out_dir=out_dir, critic_iters=5, clip_value=0.01)\n",
    "    curves['acgan'] = capture_run('acgan', epochs=epochs, num_workers=num_workers, out_dir=out_dir)\n",
    "    out_path = os.path.join(out_dir, 'samples', 'combined', 'loss_curve.png')\n",
    "    combined_loss_plot(curves, out_path)\n",
    "    print(f\"Saved combined loss plot to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
